---
output:
    html_fragment
---

<head>
  <meta charset="UTF-8">
</head>

*If you enjoy this post, subscribe using the form to the left! I try
 to make new posts every weekend, though sometimes life gets in the
 way.*

I'm interrupting my series on wavelets to do a post on something I've
been interested in doing for a while. I thought it would be a cool
"spy device" to create something that can filter out voices from a
recording of a crowded room, and I think I can do that with concepts
I've previously developed in this blog. My intuition:

1. Every person has a unique set of vocal chords that outputs a
certain signature of frequencies.

2. As that person talks, the volume at those frequencies will increase
and decrease.

3. A person's voice should be the biggest source of volume change at
these frequencies. 

**Therefore**, since [principal component analysis
  (PCA)](http://michaeljflynn.net/2017/02/06/a-tutorial-on-principal-component-analysis/)
  can pick out sets of frequencies that vary with each other, I
  believe that individuals' voices can be isolated from a crowded
  recording by dividing the recording into equally-sized blocks,
  applying the [Fourier
  transform](https://en.wikipedia.org/wiki/Fourier_transform) to each
  block (these two steps together are commonly called [short-time
  Fourier
  transform](https://en.wikipedia.org/wiki/Short-time_Fourier_transform)),
  and then doing PCA in frequency-space across all blocks. The
  individual voices should be the projection of the recording onto the
  principal components.

*Note from the future:* Turns out that I was wrong. But here's how I
 did it:

I've downloaded a `.wav` file of a crowded bar from [Freesound.org
user lonemonk](http://freesound.org/people/lonemonk/sounds/31487/).
Give it a listen to see if you can pick anything out, author notes
that there are around 50 people in the recording:

<iframe frameborder="0" scrolling="no" src="https://freesound.org/embed/sound/iframe/31487/simple/large/" width="920" height="245"></iframe>

Let's talk about the storage format. A `.wav` file is basically *raw*
sound data, meaning that the entries can be interpreted as voltages
across a microphone's sound plate. These voltages are stored in a
pre-set number of bits, are sampled a certain amount of times per
second, and typically come in 1 or 2 channels for mono or stereo
(two-eared) sound. I'll use Python to figure out these parameters for
this file, using the `wave` library. 

I've saved off the audio to a file called `crowded_bar.wav`:

```{python}
import wave
import numpy
import struct

voices = wave.open('crowded_bar.wav', 'r')

print "Number of channels: " + str(voices.getnchannels())

print "Sample bytewidth: " + str(voices.getsampwidth())

print "Framerate: " + str(voices.getframerate())

print "Number of frames: " + str(voices.getnframes())
```

So it looks like we have 2 audio channels, each sample is 2 bytes long
so we can represent each sample as a 16-bit integer, there are 44100
samples per second for a total number of 15478784 frames. That means
our recording is $15478784 \mbox{ frames}/44100 \mbox{ frames/s}
\approx 351 \mbox{ seconds}$. This agrees with the runtime of the
audio, 5 minutes and 51 seconds.

So let's start coding this up. I'm separating the recording into
10,000 chunks, each chunk with 1547 samples. 

```{python, eval=FALSE}
num_chunks = 10000
chunk_width = voices.getnframes()/num_chunks
``` 

The samples are stored in 16-bit, little endian format. Python has a
nice function called `struct.unpack` for "unpacking" this data into an
int:

```{python, eval = FALSE}
           ## unpack little-endian ints
samples =  [[struct.unpack("<h", 
                           ## read the first 2 bytes of 1 frame. The
                           ## total sample if 4 bytes long, 1 for each
                           ## sample, and since the output is a tuple
                           ## take the 0th output.
                           voices.readframes(1)[0:2])[0] 
             ## for all samples in the chunk
             for i in range(chunk_width)] 
            ## for all chunks in the recording.
            for y in range(num_chunks)]    
``` 

Now apply the Fourier transform to each block:

```{python, eval=FALSE}
print "Starting fft..."
def print_and_fft(x, i):
    print i
    return numpy.fft.fft(x)
transformed = numpy.matrix(map(print_and_fft, samples, range(len(samples))))
print "Done fft."
```

In the next step, I use only the 1st-$N/2$th elements of the Fourier
transform. The input and output of this algorithm are real numbers, so
that means that the 2nd half of the Fourier transform components are
going to be the complex conjugates of the first half, in reverse
order, and therefore are redundant. The reason I leave the 0th
component out is because that is those are the mean voltages of the
chunk, and if I leave them in, in practice I've found that when I
project onto a principle component, the mean becomes an imaginary
number. I don't want that, so I just set them to zero, effectively
filtering out the lowest frequency.

```{python, eval=FALSE}
means = [0 for x in transformed[:,0]] 
transformed = transformed[:,1:(transformed.shape[1]/2+1)]
```

The next step is the PCA, finding the eigenvectors of the covariance
matrix. See my [recent
post](http://michaeljflynn.net/2017/02/06/a-tutorial-on-principal-component-analysis/)
on PCA for an explanation of this calculation.

```{python, eval=FALSE}
mean_zero = numpy.matrix(numpy.apply_along_axis(lambda x: x - numpy.mean(x), 0, transformed))
cov = numpy.dot(mean_zero.getH(), mean_zero)
print "Starting eigenvector solve..."
w, v = numpy.linalg.eig(cov)
print "Done eigenvector solve..."
```

Now I want to project the spectrum of each chunk on to the principle
components, here's how I do that. It's basically using the formula for
projection of vector $\vec v$ onto unit vector $\hat e$: $\langle \vec
v, \hat e \rangle \hat e$.

```{python, eval = FALSE}
def projected_spectrum(component): 
    return numpy.dot(numpy.dot(transformed.conj(), v[:,component]), v.transpose()[component,:])
```

Now we want to invert these projections, and see what the output audio
is: 

```{python, eval=FALSE}
def print_and_ifft(x, i):
    print i
    return numpy.fft.ifft(x)

def write_component_to_file(component):
    projected = projected_spectrum(component)
    projected = numpy.column_stack((means, projected, numpy.fliplr(projected.conj())))
    inverted = map(print_and_ifft, projected.tolist(), range(len(projected)))
    first_component = wave.open(str(component) + '.wav', 'w')
    width = projected.shape[0]
    height = projected.shape[1]
    first_component.setparams(
        (1, # nchannels
         2, # sampwidth
         voices.getframerate(), # framerate
         width * height, 
         voices.getcomptype(),
         voices.getcompname()))
    frames = "".join(["".join([struct.pack("<h", int(i.real)) for i in x]) for x in inverted])     
    first_component.writeframes(frames)
    first_component.close()
    print "Wrote component " + str(component) + " to file."
```

Now let's write the first 5 components to a file, are they voices?

```{python, eval=FALSE}
write_component_to_file(0)
write_component_to_file(1)
write_component_to_file(2)
write_component_to_file(3)
write_component_to_file(4)
```

Nope!! 

<iframe frameborder="0" scrolling="no" src="https://freesound.org/embed/sound/iframe/397383/simple/large/" width="920" height="245"></iframe>

<iframe frameborder="0" scrolling="no" src="https://freesound.org/embed/sound/iframe/397382/simple/large/" width="920" height="245"></iframe>

<iframe frameborder="0" scrolling="no" src="https://freesound.org/embed/sound/iframe/397381/simple/large/" width="920" height="245"></iframe>

<iframe frameborder="0" scrolling="no" src="https://freesound.org/embed/sound/iframe/397380/simple/large/" width="920" height="245"></iframe>

<iframe frameborder="0" scrolling="no" src="https://freesound.org/embed/sound/iframe/397384/simple/large/" width="920" height="245"></iframe>


After fiddling around for a night with the settings, I've decided that
my intuition was wrong. People's voices don't stick to a fixed set of
frequencies, their voices change frequencies as they speak all the
time. For example, a long, sinking *oooooooohhh* sound exhibits a
pretty constant downward drift in frequency. Since everybody's vocal
frequencies are constantly changing and crossing paths with each
other, I don't think you can isolate one voice by looking at a
constant cross section of frequencies. The problem is harder than
that.

After doing some research I've discovered that this is indeed a pretty
hard problem in signal processing, called [blind signal
seperation](https://en.wikipedia.org/wiki/Blind_signal_separation),
also known as the cocktail party problem. Apparently [deep
learning](https://www.technologyreview.com/s/537101/deep-learning-machine-solves-the-cocktail-party-problem/)
has been used to solve the problem. That's interesting, but I somewhat
resent a "black box" solution. I'm going to table my exploration of
this for now but was interesting to work with audio data.

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>



